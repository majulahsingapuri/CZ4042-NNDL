{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXnCczN5vwc7"
   },
   "source": [
    "# Neural Networks and Deep Learning\n",
    "---\n",
    "\n",
    "The use of various neural network architectures for achieving desired outcomes has been a key factor leading to their incorporation into a wide array of tasks and industries. In this information age, we have access to a lot of information and at the same time need to consume as much of it as possible, thus requiring a system that can summarise the data concisely.\n",
    "\n",
    "News is one such source of data that most people turn to to keep up to date about the happenings around the world. The headlines of news articles must be concise enough to get as much key information to the reader in as short a sentence as possible, thus making it ideal for training a summarisation engine.\n",
    "\n",
    "Additionally, categorising the information based on the content is equally as necessary as the summary itself as different people might want to know about a different subset of information as compared to other people.\n",
    "\n",
    "Therefore, in this project, we will be looking at 2 models. The first is a categorisation model that seeks to categorise news articles and the second is one that aims to summarise the content to extract its meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pf2Xv9xjvwc-"
   },
   "source": [
    "## Importing Libraries\n",
    "\n",
    "First, we need to download `spaCy`, which is a NLP library that has pretrained word vectors that capture information about the words themselves, thus making it easier for us to train our final model. If you are running this on Google Colab, you will have to restart the kernel after installation for the changes to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJdLOp2wb8lY"
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjjaa1imvwdB"
   },
   "source": [
    "Next we will be importing all the things that we will be using in this project. If you are missing any libraries do install them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQAOsNPP7XKa",
    "outputId": "3cf72446-0c6c-4518-e9ab-f493e490dd8b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from matplotlib.colors import LogNorm\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from json import load, dump\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras, constant\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Layer, Dense, Input, LSTM, TimeDistributed, Concatenate, Attention, Embedding, Bidirectional, Dropout, BatchNormalization, Masking\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.initializers import Constant, GlorotUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "nltk.download('stopwords')\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.get_logger().setLevel('WARNING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lVzHsV3vwdD"
   },
   "source": [
    "We also need to check if we have a GPU installed and configured to speed up the learning process later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sxa3uQFSxkt0",
    "outputId": "ec217a43-a4a8-4132-9000-6a6410256d4f"
   },
   "outputs": [],
   "source": [
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WQcaHDJ14T9"
   },
   "source": [
    "We also need to set some parameters here so that it is easy to vary them later on for fine tuning of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_KT8AoU0DVj"
   },
   "outputs": [],
   "source": [
    "# Category Model Params\n",
    "category_model_name = 'CategorisationModel'\n",
    "embedding_dim = 300\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Summary Model Params\n",
    "summary_model_name = 'SummarisationModel'\n",
    "batch_size = 256\n",
    "latent_dim = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kpqz4F847XKh"
   },
   "source": [
    "## Data Preparation and Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5X5xGiJ8a5m"
   },
   "source": [
    "### Reading the Data\n",
    "\n",
    "Firstly we need to import the data, which is in a JSON file. This dataset was retrieved from [Kaggle](https://www.kaggle.com/rmisra/news-category-dataset?select=News_Category_Dataset_v2.json), which we have expanded upon and [scraped](https://gist.github.com/majulahsingapuri/535ac8d3daac708996a3588e5c9d18e6) the actual articles for their first 3 paragraphs of content. We chose this dataset as it had over 200k data points, which is more than enough data for our needs, and it also provided links to the original articles, making it easier for us to scrape them for their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "LVJmfN9U7XKj",
    "outputId": "0b66c9fc-6c30-48c6-c584-2383df2bd62a"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_json('./data/News_Category_Dataset_pretty_v3.json', orient = 'records')\n",
    "df = df.drop(columns=['authors', 'link', 'date', 'short_description'])\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Id42mjM3lmD"
   },
   "source": [
    "In the process of scraping we did lose a few thousand data points as the articles were either removed or not accessible but it is still more than enough for us to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0l35CpezvwdG",
    "outputId": "a069a35e-6563-4d06-c1c6-83ba85c95a60"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBH0jja1vwdG"
   },
   "source": [
    "### Article Categories\n",
    "Next, lets take a look at how many categories we have to categorise the news articles into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qvf0A-MH7XKk",
    "outputId": "b3a13ff0-a7bb-4d15-9014-704fc859f027"
   },
   "outputs": [],
   "source": [
    "cates = df.groupby('category')\n",
    "print(\"total categories:\", cates.ngroups)\n",
    "print(cates.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blwMr8jRvwdG"
   },
   "source": [
    "Replacing `THE WORLDPOST` with `WORLDPOST` as they are in essence the same tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5We2gtX7XKl"
   },
   "outputs": [],
   "source": [
    "# as shown above, THE WORLDPOST and WORLDPOST should be the same category, so merge them.\n",
    "df.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxK2mXn8vwdH"
   },
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Now to actually process the data. First, since the content has been scraped from the internet, in the event that some html code has made its way into the dataset, we will have to remove it. In addtion, we will also be removing other text fragments as highlighted in the code comments below. We will also remove remove all contractions to ensure that the data is written in as proper english as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpMk1zCwLz5O"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKZvaS4ezWL6"
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    newString = text.lower() # Lower Case\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text # Remove html fragments\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString) # Anything in brackets\n",
    "    newString = re.sub('\"','', newString) # Quotes\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")]) # Contractions   \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString) # Possessive apostrophe\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) # Non-letters\n",
    "    newString = re.sub('[m]{2,}', 'mm', newString) # \"Hmmm\" sort of letters?\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghYYa1FyzXXQ"
   },
   "outputs": [],
   "source": [
    "df['content'] = df.content.apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnz-Snng9gZ_"
   },
   "source": [
    "### Visualising Data\n",
    "Next, since we will be using LSTMs and Embedding layers, we need to know what are the input and output sizes that we will be dealing with so that we can build the model appropriately. The chart below shows us that a majority of the content is about 100-200 words long and the headlines are 12-14 words long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r92v0ME-zcML",
    "outputId": "8cccda16-74af-4520-dd58-bde664420dd2"
   },
   "outputs": [],
   "source": [
    "text_word_count = []\n",
    "headline_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in df['content']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in df['headline']:\n",
    "      headline_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'content':text_word_count, 'headline':headline_word_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_list = length_df['content'].hist(bins = 1000)\n",
    "ax_list.set_xlim(0, 500)\n",
    "plt.savefig('./Images/content_distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_list = length_df['headline'].hist(bins = 50)\n",
    "plt.savefig('./Images/headline_distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK7kKmmf-RJy"
   },
   "source": [
    "Hence we will be setting the `max_text_length` and `max_headline_length` to the following values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amnVyWJLzcJg"
   },
   "outputs": [],
   "source": [
    "max_text_len=200\n",
    "max_headline_len=14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsrY-S2Y-bns"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "Next we need to add the start and end tokens into the headlines in order for the decoder portion of the summary LSTM model to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiMY35GxzcGu"
   },
   "outputs": [],
   "source": [
    "df['headline'] = df['headline'].apply(lambda x : 'sostok '+ x + ' eostok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TTdRsic-3KF"
   },
   "source": [
    "We will also be trimming all the articles with headlines below `max_headline_len` number of words to ensure that every headline has a start and end token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfiLQ6TBzcD2"
   },
   "outputs": [],
   "source": [
    "df = df[df['headline'].str.split().str.len().le(max_headline_len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIeXv2Fn_n68"
   },
   "source": [
    "Next we need to convert the categories from text to numbers so that they are easier to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svk1TTWI7XKq"
   },
   "outputs": [],
   "source": [
    "df['category'], categories = pd.factorize(pd.Categorical(df['category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQi1SWcsHOj8"
   },
   "source": [
    "### Splitting the Data\n",
    "\n",
    "We will be using `sklearn`'s `train_test_split` to split the data into `train`, `test` and `validation` sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOGrs2Xfzb97"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df['content'],\n",
    "    df[['headline', 'category']],\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    test_size=0.25,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLCfqJHNzb7Q",
    "outputId": "6a507025-6478-465c-8887-0d991b35bb99"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgdOABccIVX9"
   },
   "source": [
    "### Tokenising Data\n",
    "\n",
    "Now we need to fit the `Tokenizer`s onto the `x_train` and `y_train` data create our bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae4MqyLZzb39"
   },
   "outputs": [],
   "source": [
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8NZrodrIoL1"
   },
   "source": [
    "Rather than covering all the words that exist in the dataset, we want to focus on the ones that appear more often as that will aid in the training of the model. We are taking all words that appear at least 4 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RM3GIlz7zb1X",
    "outputId": "e4579c5c-8d6b-44c1-d878-a3c1bacd16e6"
   },
   "outputs": [],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6metrG5JE6P"
   },
   "source": [
    "Now we will refit the data on the reduced vocabulary size on each of the `train`, `test` and `validation` datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNlUjT-W0Dok"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_train))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_train    =   x_tokenizer.texts_to_sequences(x_train) \n",
    "x_test   =   x_tokenizer.texts_to_sequences(x_test)\n",
    "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_train    =   pad_sequences(x_train,  maxlen=max_text_len, padding='post')\n",
    "x_test   =   pad_sequences(x_test, maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HljYQnvJWtw"
   },
   "source": [
    "Checking size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoV--PBu0Dls",
    "outputId": "2ed3df5c-b5d9-4314-c780-fcd3d38f0fe4"
   },
   "outputs": [],
   "source": [
    "x_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9c023UrJTcS"
   },
   "source": [
    "Checking to see if the padding has been done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "zRTi_5A87XKm",
    "outputId": "ef932e3f-a278-400c-ce68-7fc3a15e51df"
   },
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAMfKKeJJau5"
   },
   "source": [
    "Repeat the process for `y` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JguhhXc0DjM"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_train['headline']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6k5GDPUI0DgZ",
    "outputId": "a4189952-2638-4ccb-e8d7-33510ba8300e"
   },
   "outputs": [],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XH5OptQe0Ddt"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_train['headline']))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "y_train_seq    =   y_tokenizer.texts_to_sequences(y_train['headline']) \n",
    "y_test_seq   =   y_tokenizer.texts_to_sequences(y_test['headline']) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val['headline']) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_train_seq    =   pad_sequences(y_train_seq, maxlen=max_headline_len, padding='post')\n",
    "y_test_seq   =   pad_sequences(y_test_seq, maxlen=max_headline_len, padding='post')\n",
    "y_val_seq   =   pad_sequences(y_val_seq, maxlen=max_headline_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAgGIrmfvwdL"
   },
   "outputs": [],
   "source": [
    "y_train = y_train.drop(columns='headline')\n",
    "y_test = y_test.drop(columns='headline')\n",
    "y_val = y_val.drop(columns='headline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c49jhFtX7XKt"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(list(y_train['category']))\n",
    "y_test = to_categorical(list(y_test['category']))\n",
    "y_val = to_categorical(list(y_val['category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrF9sPvvJgyG"
   },
   "source": [
    "Checking to see if the data has been correctly categorised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wv3b4UDMvwdM",
    "outputId": "0c1c80e7-b776-4686-841f-c3793a92c46a"
   },
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3G7P6AYQJnI5"
   },
   "source": [
    "Checking to see if the padding has been done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jnFy3exvwdM",
    "outputId": "52887b80-f48a-4fbd-bd93-bdf0e2bb4948",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxcZcRrIJso0"
   },
   "source": [
    "### Saving Tokenizers for future use\n",
    "In the event that we want to deploy this model, we need to save the tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38eAAhOa0DbB"
   },
   "outputs": [],
   "source": [
    "tokenizer_json = x_tokenizer.to_json(ensure_ascii=False, indent=4)\n",
    "with open('./tokenizer/xtokenizer.json', 'w+', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9AN2_We0DYJ"
   },
   "outputs": [],
   "source": [
    "tokenizer_json = y_tokenizer.to_json(ensure_ascii=False, indent=4)\n",
    "with open('./tokenizer/ytokenizer.json', 'w+', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xscdtgau7XKr"
   },
   "source": [
    "## Categorising Articles\n",
    "Now lets begin building the first model that categorises news articles based on its content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYjNqFo2J_En"
   },
   "source": [
    "### Loading spaCy\n",
    "We will be using [spaCy](https://spacy.io) for this part as it is the state of the art model for NLP processing. It contains many pipelines that we have had to disable as we are only interested in the `word_2_vect` pipe, which converts words to their vector representation. These vectors have been derived after the spacy model has been trained on an extremely large dataset to accurately encapsulate the meanings of these words into a 300-dimensional vector. We will use this for our Embedding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEzNxne5ghQA"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg', exclude=['tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5NKmxZN7XKs",
    "outputId": "6ee9b08e-3c9b-4de7-c582-f3bbbba73a93"
   },
   "outputs": [],
   "source": [
    "x_word_index = x_tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(x_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHgVCwpw7XKs"
   },
   "outputs": [],
   "source": [
    "x_embedding_matrix = np.zeros((len(x_word_index) + 1, 300))\n",
    "for word, i in x_word_index.items():\n",
    "    embedding_word = nlp(word)\n",
    "    embedding_vector = embedding_word.vector\n",
    "    if embedding_vector is not None:\n",
    "        x_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_word_index = y_tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(y_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_embedding_matrix = np.zeros((len(y_word_index) + 1, 300))\n",
    "for word, i in y_word_index.items():\n",
    "    embedding_word = nlp(word)\n",
    "    embedding_vector = embedding_word.vector\n",
    "    if embedding_vector is not None:\n",
    "        y_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Embeddings for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./embeddings/xEmbedding.npy', 'wb') as f:\n",
    "    np.save(f, x_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./embeddings/yEmbedding.npy', 'wb') as f:\n",
    "    np.save(f, y_embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bospvyX7XKx"
   },
   "source": [
    "### Attention Layer\n",
    "\n",
    "For this Model, we will not be using the TensorFlow Attention layer but rather th is custom implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dvra7S8YvwdN"
   },
   "outputs": [],
   "source": [
    "class CustomAttention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = GlorotUniform\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(CustomAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        base_config = super().get_config()\n",
    "        config = {\n",
    "            'step_dim' : self.step_dim,\n",
    "            'W_regularizer' : self.W_regularizer,\n",
    "            'b_regularizer' : self.b_regularizer,\n",
    "            'W_constraint' : self.W_constraint,\n",
    "            'b_constraint' : self.b_constraint,\n",
    "            'bias' : self.bias\n",
    "        }\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name=f'{self.name}_W',\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name=f'{self.name}_b',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utgTBh8LPSeZ"
   },
   "source": [
    "### Building the Model\n",
    "\n",
    "Now we will combine all the different layers into the model shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ou7Azb77XKy",
    "outputId": "2cf6d0b7-23d5-41a2-b37a-7a92c7adb5b9"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "inp = Input(shape=(max_text_len,), dtype='int32')\n",
    "x = Embedding(len(x_word_index)+1, embedding_dim, embeddings_initializer=Constant(x_embedding_matrix), input_length=max_text_len, trainable=False, mask_zero=True)(inp)\n",
    "x = Bidirectional(LSTM(embedding_dim, dropout=0.25, return_sequences=True))(x)\n",
    "x = CustomAttention(max_text_len)(x)\n",
    "merged = Dense(256, activation='relu')(x)\n",
    "merged = Dropout(0.25)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "outp = Dense(len(categories), activation='softmax')(merged)\n",
    "\n",
    "AttentionLSTM = Model(inputs=inp, outputs=outp, name=category_model_name)\n",
    "AttentionLSTM.compile(loss=CategoricalCrossentropy(), optimizer=Adam(learning_rate=learning_rate), metrics=[CategoricalAccuracy(name='acc')])\n",
    "\n",
    "AttentionLSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRWiuEAyQeyA"
   },
   "source": [
    "### Visualise the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEX6cFtwvwdN",
    "outputId": "03bc72b2-971c-415c-d967-e3a337adb2e1"
   },
   "outputs": [],
   "source": [
    "plot_model(AttentionLSTM, to_file='./Images/' + category_model_name + '.png', rankdir='TB', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIb-vcHrQlD_"
   },
   "source": [
    "### Setting the Callbacks\n",
    "\n",
    "We have implemented an `EarlyStopping` and `ModelCheckpoint` callback which will allow us to obtain the most optimal model for our uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4d1SLeJ0DKl"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    './models/' + category_model_name + '.h5', \n",
    "    monitor='val_loss',  \n",
    "    mode='min', \n",
    "    verbose=1, \n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6Ux7pSVRfnK"
   },
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saAwsiic7XKy",
    "outputId": "fdec6d5f-b7b0-4a3a-b0bf-ac253527fbe9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "attlstm_history = AttentionLSTM.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    callbacks=[es, checkpoint],\n",
    "    batch_size=batch_size, \n",
    "    epochs=50, \n",
    "    validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh1RkYhiRuX0"
   },
   "source": [
    "### Plotting the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "eHbddGAA7XKy",
    "outputId": "63261bd2-7c06-4d91-ef69-50c314979299"
   },
   "outputs": [],
   "source": [
    "acc = attlstm_history.history['acc']\n",
    "val_acc = attlstm_history.history['val_acc']\n",
    "loss = attlstm_history.history['loss']\n",
    "val_loss = attlstm_history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Training and validation accuracy')\n",
    "plt.plot(epochs, acc, 'red', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n",
    "plt.legend()\n",
    "plt.savefig('./Images/categorisation_accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Training and validation loss')\n",
    "plt.plot(epochs, loss, 'red', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.savefig('./Images/categorisation_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAsjWs19btpM"
   },
   "source": [
    "### Loading the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6JaNvegblkB"
   },
   "outputs": [],
   "source": [
    "AttentionLSTM = load_model('./models/' + category_model_name + '.h5', custom_objects={\n",
    "    'CustomAttention': CustomAttention\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mohTrV5-by5p"
   },
   "source": [
    "### Plotting the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uEW3iS97XKz"
   },
   "outputs": [],
   "source": [
    "predicted = AttentionLSTM.predict(x_val)\n",
    "cm = confusion_matrix(y_val.argmax(axis=1), predicted.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sb.heatmap(cm, annot=True, square=True, norm=LogNorm())\n",
    "plt.savefig('./Images/categorisation_confusion_matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "S_x9pfn07XKz"
   },
   "source": [
    "### Evaluating the Model\n",
    "We will now use the `validation` dataset to validate our model on data it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PzA4CY-o7XK0",
    "outputId": "137723b4-d8e0-4d58-aa75-5b22e7ba8548"
   },
   "outputs": [],
   "source": [
    "AttentionLSTM.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRd1FkGOzEcr"
   },
   "source": [
    "## Summarising the content\n",
    "Now we will build our second model, which is the summary model. This model requires multiple layers of LSTMs as we need to gather information from the entirety of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ndF6Ei30DTA",
    "outputId": "1cba8a54-7998-4ee7-b869-28cd8adcee24"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len, ))\n",
    "\n",
    "# Embedding layer\n",
    "enc_emb =  Embedding(len(x_word_index)+1, embedding_dim, embeddings_initializer=Constant(x_embedding_matrix), input_length=max_text_len, trainable=False, mask_zero=True)(encoder_inputs)\n",
    "\n",
    "# Encoder LSTM 1\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# Encoder LSTM 2\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# Encoder LSTM 3\n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True, dropout=0.2)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as the initial state\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "# Embedding layer\n",
    "dec_emb_layer = Embedding(len(y_word_index)+1, embedding_dim, embeddings_initializer=Constant(y_embedding_matrix), input_length=max_text_len, trainable=False, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Decoder LSTM\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2)\n",
    "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_out = Attention()([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=summary_model_name)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNHklvGN0DP7",
    "outputId": "3a0e5f7c-f410-4e43-d454-32db726d6720"
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='./Images/' + summary_model_name + '.png', rankdir='TB', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTDv7Bb80DNb"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=RMSprop(), loss=SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhotdJpbXoqx"
   },
   "source": [
    "### Creating the Datasets\n",
    "\n",
    "We will be using the Dataset class to load the data and send it to the appropriate inputs in our model. The first input is the entire 100 word content and the second input is entire headline minus the last token. The output is the entire headline minus the first token, as individual outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IQKvRUi0DHk"
   },
   "outputs": [],
   "source": [
    "train_data = Dataset.from_tensor_slices(({\n",
    "        \"input_1\": x_train, \n",
    "        \"input_2\": y_train_seq[:,:-1]\n",
    "    }, \n",
    "    y_train_seq.reshape(y_train_seq.shape[0], y_train_seq.shape[1], 1)[:,1:]\n",
    ")).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_tjGa-v0DEw"
   },
   "outputs": [],
   "source": [
    "test_data = Dataset.from_tensor_slices(({\n",
    "        \"input_1\": x_test, \n",
    "        \"input_2\": y_test_seq[:,:-1]\n",
    "    }, \n",
    "    y_test_seq.reshape(y_test_seq.shape[0], y_test_seq.shape[1], 1)[:,1:]\n",
    ")).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtTS-Lygc5G6"
   },
   "outputs": [],
   "source": [
    "val_data = Dataset.from_tensor_slices(({\n",
    "        \"input_1\": x_val, \n",
    "        \"input_2\": y_val_seq[:,:-1]\n",
    "    }, \n",
    "    y_val_seq.reshape(y_val_seq.shape[0], y_val_seq.shape[1], 1)[:,1:]\n",
    ")).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gQ90WRAa62M"
   },
   "source": [
    "### Setting the Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_Jg19YUvwdP"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    './models/' + summary_model_name + '.h5', \n",
    "    monitor='val_loss',  \n",
    "    mode='min', \n",
    "    verbose=1, \n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNxIOGW7a-1u"
   },
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EA9sHvO0DCQ",
    "outputId": "7dd26a81-2672-47d3-83bf-05f252eccc45",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=1,\n",
    "    callbacks=[es,checkpoint],\n",
    "    batch_size=batch_size, \n",
    "    validation_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjF9rzoNbKli"
   },
   "source": [
    "### Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCwJYrhP0C_Q",
    "outputId": "2b6650e2-a5bc-4c1c-d5ed-76fd74c8e32a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.savefig('./Images/summary_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPxK6qx2bSgp"
   },
   "source": [
    "### Loading the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qCOQyvRzbyk"
   },
   "outputs": [],
   "source": [
    "model = load_model('./models/' + summary_model_name + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aa1gBnlWcTc9"
   },
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8lxo4HZcW6J"
   },
   "outputs": [],
   "source": [
    "model.evaluate(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1ENludwb99p"
   },
   "source": [
    "### Building the Encoder and Decoder Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUqLp0_Fzbrp"
   },
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xXl5kOK0cqB"
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# Attention inference\n",
    "attn_out_inf = Attention()([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSBw0f5ycCaU"
   },
   "source": [
    "#### Saving the Encoder and Decoder Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbnYTZs00cnf",
    "outputId": "8cfedb0a-4e5d-45fd-f556-36dd16ecdbc3"
   },
   "outputs": [],
   "source": [
    "encoder_model.save('./models/encoder_' + summary_model_name + '.h5')\n",
    "decoder_model.save('./models/decoder_' + summary_model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(encoder_model, to_file='./Images/encoder_model.png', rankdir='TB', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(decoder_model, to_file='./Images/decoder_model.png', rankdir='TB', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ab7k-vPcILd"
   },
   "source": [
    "### Functions to predict the headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYgJ3qKQ0ck-"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_headline_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj4ikJo20ciY"
   },
   "outputs": [],
   "source": [
    "def seq2headline(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_4ma5T10cf_"
   },
   "outputs": [],
   "source": [
    "with open('./outputs/' + summary_model_name + '.txt', 'w') as f:\n",
    "    for i in range(0,50):\n",
    "        f.write(\"Article:\" + seq2text(x_val[i]) + '\\n')\n",
    "        f.write(\"Original headline:\" + seq2headline(y_val_seq[i]) + '\\n')\n",
    "        f.write(\"Predicted headline:\" + decode_sequence(x_val[i].reshape(1,max_text_len)) + '\\n')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTm5UZRzcNv2"
   },
   "source": [
    "### Creating headlines from custom text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veALLzLK0cdg"
   },
   "outputs": [],
   "source": [
    "# Need to add the category to this too\n",
    "def create_headline(_input):\n",
    "    _input = x_tokenizer.texts_to_sequences(_input)\n",
    "    _input = pad_sequences(_input,  maxlen=max_text_len, padding='post')\n",
    "    return decode_sequence(_input.reshape(1,max_text_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFdbCKmV0kl-"
   },
   "outputs": [],
   "source": [
    "text = '''\n",
    "SINGAPORE: From Dec 8, all COVID-19 patients who are unvaccinated \"by choice\" will have to pay their own medical bills if they are admitted to hospitals or COVID-19 treatment facilities, the Ministry of Health (MOH) said on Monday (Nov 8).\n",
    "\n",
    "The Government is currently footing the full COVID-19 medical bills of all Singaporeans, permanent residents and long-term pass holders, other than for those who test positive soon after returning from overseas travel.\n",
    "\n",
    "\"Currently, unvaccinated persons make up a sizeable majority of those who require intensive inpatient care, and disproportionately contribute to the strain on our healthcare resources,\" said MOH.\n",
    "\n",
    "'''\n",
    "\n",
    "create_headline([text])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NNDL_Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
